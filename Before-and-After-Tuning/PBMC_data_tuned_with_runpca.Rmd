---
title: "PBMC Dataset - Tuned Parameters with runPCA optimization"
output: html_notebook
---
# Loading Libraries

The first task is to load required libraries.

```{r message=FALSE, warning=FALSE}
library(dropClust)
library(scater)
library(SingleCellExperiment)
library(clValid)
library(ggplot2)
library(stats)
library(PCAtools)
set.seed(123)
```

# Loading Data
Then, we should load the count matrix into memory and convert it to SingleCellExperiment object.

```{r}
sce <- SingleCellExperiment(list(counts=data_PBMC))
sce
```

```{r}
dim(sce)
class(sce)
```

# Quality control on the cells and genes

Low-quality cells need to be removed to ensure that technical effects do not distort downstream analysis results. Two
common measures of cell quality are the library size and the number of expressed features in each library.

```{r}
# Find expressed cell per gene
# colSums(assay(sce))
# Find expressed gene per cell
# rowSums(assay(sce))
```

```{r}
# find non-zero counts
nonZero <- (counts(sce) > 0)

# find rows/genes with at least one non-zero count 
keep <- (rowSums(nonZero) > 0)

# keep only the genes with non-zero count the SCE object 
sce_2 <- sce[keep, ]

# explore sce_2
sce_2
```


```{r}
dim(sce_2)
```

## Quality Control Metrics

Low-quality cells need to be removed to ensure that technical effects do not distort downstream analysis results. Two common measures of cell quality are the library size and the number of expressed features in each library. The library size is defined as the total sum of counts across all features. Cells with relatively small library sizes are considered to be of low quality as the RNA has not been efficiently captured during library preparation. The number of expressed features in each cell is defined as the number of features with non-zero counts for that cell. Any cell with very few expressed genes is likely to be of poor quality as the diverse transcript population has not been successfully captured.

For each cell, we calculate quality control metrics such as the total number of counts.*perCellQCMetrics* function calculates useful QC metrics for identification and removal of potentially problematic cells. Obvious per-cell metrics are the sum of counts (i.e., the library size) and the number of detected features. The percentage of counts in the top features also provides a measure of library complexity.

```{r}
# is.mito <- grepl("^MT-", rownames(sce_2))
# sce_2 <- perCellQCMetrics(sce_2,subsets=list(Mito=is.mito))
sce_2 <- perCellQCMetrics(sce_2)
df <- data.frame(library_sizes=sce_2$sum,number_expressed=sce_2$detected)
ggplot(df,aes(library_sizes)) + geom_histogram(binwidth=100,colour = "black",fill="orange") + xlab("Library sizes") + ylab("Number of cells") +
  ggtitle("Histogram")
```

```{r}
ggplot(df,aes(library_sizes)) + geom_histogram(aes(y= ..density..),binwidth = 100,colour = "black",fill="orange") + geom_density(alpha=.3,fill="black") +
  ggtitle("Histogram + Density")
```


```{r}
ggplot(df,aes(number_expressed)) + geom_histogram(binwidth=100,colour = "black",fill="orange") + xlab("Number of expressed genes") + ylab("Number of cells") +
  ggtitle("Histogram")
```

```{r}
ggplot(df,aes(number_expressed)) + geom_histogram(aes(y= ..density..),binwidth = 100,colour = "black",fill="orange") + geom_density(alpha=.3,fill="black") +
  ggtitle("Histogram + Density")
```

Alternatively, users may prefer to use the addPerCellQC() function. This computes and appends the per-cell QC statistics to the colData of the SingleCellExperiment object, allowing us to retain all relevant information in a single object for later manipulation.

```{r}
# sce_2 <- addPerCellQC(sce_2, subsets=list(Mito=is.mito))
# colnames(colData(sce_2))
```

A key assumption here is that the QC metrics are independent of the biological state of each cell. Poor values (e.g., low library sizes, high mitochondrial proportions) are presumed to be driven by technical factors rather than biological processes, meaning that the subsequent removal of cells will not misrepresent the biology in downstream analyses. Major violations of this assumption would potentially result in the loss of cell types that have, say, systematically low RNA content or high numbers of mitochondria.

## Identifying low-quality cells with fixed thresholds

The simplest approach to identifying low-quality cells is to apply thresholds on the QC metrics. For example, we might consider cells to be low quality if they have library sizes below 2,000 reads; express fewer than 500 genes;

```{r}
qc.lib <- sce_2$sum < 2300
qc.nexprs <- sce_2$detected < 750
# qc.mito <- sce_2$subsets_Mito_percent > 3
# discard <- qc.lib | qc.nexprs | qc.mito
# DataFrame(LibSize=sum(qc.lib),NExprs=sum(qc.nexprs), MitoProp=sum(qc.mito), Total=sum(discard))
discard <- qc.lib | qc.nexprs
# Summarize the number of cells removed for each reason.
DataFrame(LibSize=sum(qc.lib),NExprs=sum(qc.nexprs), Total=sum(discard))
```

While simple, this strategy requires considerable experience to determine appropriate thresholds for each experimental protocol and biological system. Thresholds for read count-based data are simply not applicable for UMI-based data, and vice versa. Differences in mitochondrial activity or total RNA content require constant adjustment of the mitochondrial and spike-in thresholds, respectively, for different biological systems. Indeed, even with the same protocol and system, the appropriate threshold can vary from run to run due to the vagaries of cDNA capture efficiency and sequencing depth per cell.

## Identifying low-quality cells with adaptive thresholds

### Identifying outliers

To obtain an adaptive threshold, we assume that most of the dataset consists of high-quality cells. We then identify cells that are outliers for the various QC metrics, based on the median absolute deviation (MAD) from the median value of each metric across all cells. Specifically, a value is considered an outlier if it is more than 3 MADs from the median in the “problematic” direction. This is loosely motivated by the fact that such a filter will retain 99% of non-outlier values that follow a normal distribution.

A log-transformation is used to improve resolution at small values when type="lower". Specifically, it guarantees that the threshold is not a negative value, which would be meaningless for a non-negative metric. Furthermore, it is not uncommon for the distribution of library sizes to exhibit a heavy right tail; the log-transformation avoids inflation of the MAD in a manner that might compromise outlier detection on the left tail. (More generally, it makes the distribution seem more normal to justify the 99% rationale mentioned above.)

```{r}
qc.lib2 <- isOutlier(sce_2$sum, log=TRUE, type="lower")
qc.nexprs2 <- isOutlier(sce_2$detected, log=TRUE, type="lower")
# qc.mito2 <- isOutlier(sce_2$subsets_Mito_percent, type="higher")
# attr(qc.mito2, "thresholds")
attr(qc.lib2, "thresholds")
attr(qc.nexprs2, "thresholds")
```

isOutlier() will also return the exact filter thresholds for each metric in the attributes of the output vector. These are useful for checking whether the automatically selected thresholds are appropriate.

## Filter cells with small library size with distribution plot

```{r}
threshold <- 1931.513
ggplot(df,aes(library_sizes)) + geom_density(colour = "blue",fill="white") +
  geom_vline(xintercept=threshold,color = "red") +
  xlab("Library Sizes") + ylab("Density") +
  ggtitle("Distribution")
```


```{r}
# determine the cells to keep
keep1 <- (sce_2$sum > threshold )

# tabulate the cells that were kept on the previous step
table(keep1)
```

## Filter cells by number of expressed genes with distribution plot

```{r}
threshold <- 601.4995
ggplot(df,aes(number_expressed)) + geom_density(colour = "blue",fill="white") +
  geom_vline(xintercept=threshold,color = "red") +
  xlab("Number of expressed genes") + ylab("Density") +
  ggtitle("Distribution")
```


```{r}
# determine the cells to keep
keep2 <- (sce_2$detected > threshold )

# tabulate the cells that were kept on the previous step
table(keep2)
```

```{r}
# set threshold
## threshold <- 2.352649

# plot density
## plot(density(sce_2$subsets_Mito_percent), main = 'Density - subset_mito_percent')
## abline(v = threshold)

# determine the cells to keep
## keep3 <- (sce_2$subsets_Mito_percent < threshold )

# tabulate the cells that were kept on the previous step
## table(keep3)
```


A cell that is an outlier for any of these metrics is considered to be of low quality and discarded.

```{r}
## discard2 <- qc.lib2 | qc.nexprs2 | qc.mito2
discard2 <- qc.lib2 | qc.nexprs2

# Summarize the number of cells removed for each reason.
## DataFrame(LibSize=sum(qc.lib2), NExprs=sum(qc.nexprs2), MitoProp=sum(qc.mito2), Total=sum(discard2))
DataFrame(LibSize=sum(qc.lib2), NExprs=sum(qc.nexprs2), Total=sum(discard2))
```

Let's now filter the cells

```{r}
dim(sce)
sce_3<-sce[,!discard] # We will use manually threshold value
dim(sce_3)
```

8.381 - 8368 = 13

## Filtering out low-abundance genes with log10 average count

Low-abundance genes are problematic as zero or near-zero counts do not contain enough information for reliable statistical inference. In addition, the discreteness of the counts may interfere with downstream statistical procedures, e.g., by compromising the accuracy of continuous approximations. Here, low-abundance genes are defined as those with an average count below a filter threshold of -2. These genes are likely to be dominated by drop-out events, which limits their usefulness in later analyses. Removal of these genes mitigates discreteness and reduces the amount of computational work without major loss of information.

```{r}
ave.counts <- rowMeans(counts(sce_3))
hist(log10(ave.counts), breaks=100, main="", col="grey80",
     xlab=expression(Log[10]~"average count"))
abline(v=log10(0.01), col="blue", lwd=2, lty=2)
```

```{r}
keep4 <- ave.counts >= 0.01
sum(keep4)
table(keep4)
sce<-sce_3[keep4, ]
dim(sce)
```

15.601 - 10.902 = 4.699

To check whether the chosen threshold is suitable, we examine the distribution of log-means across all genes. The peak represents the bulk of moderately expressed genes while the rectangular component corresponds to lowly expressed genes. The filter threshold should cut the distribution at some point along the rectangular component to remove the majority of low-abundance genes.

# DropClust

dropClust - Fast execution time, clustering accuracy and detectability of minor cell sub-types.

## Pre-processing

FilterCells() -> Keep only those cells (columns) expressing at least count = min_count in the number of genes specified within the quantile range between ql_th and qh_th

FilterGenes() -> Filter genes with at least a count of min_count in at least min_cell cells

Here was done in previous section

```{r}
# sce<-FilterCells(sce, min_count = 3, ql_th = 0.001, qh_th = 1)
# sce<-FilterGenes(sce, min_count = 2, min_cell = 3)
```

## Data normalization and removing poor quality genes

Compute normalized expression values from count data in a SingleCellExperiment object, using the median normalized total count stored in the object.

```{r}
set.seed(123)
sce_norm<-CountNormalize(sce,return_log = FALSE)
sce_norm<-logNormCounts(sce_norm) # scater
normcounts<-list(normcounts(sce_norm))
```

Normalized expression values are computed by dividing the counts for each cell by median normalized total count of that cell. If log=TRUE, log-normalized values are calculated by adding a pseudocount of 1 to the normalized count and performing a log2 transformation.

## Selecting highly variable genes

Get variable genes from normalized UMI counts using Fano Factor metric.

```{r}
# Select Top Dispersed Genes by setting ngenes_keep.
set.seed(123)
sce_rank<-RankGenes(sce_norm, ngenes_keep = 1225)
keep_highly_variable_gene<-rowData(sce_rank)[,"HVG"]
sce_HVG<-sce_rank[keep_highly_variable_gene, ]
dispersion_norm<-rowData(sce_HVG)[,"dispersion_norm"]
```

Compute Fano Factor metric for each gene. The metric computes the median absolute deviation of dispersion across multiple bins for each gene.

A SingleCellExperiment object with an additional column named HVG in rowData column. The column stores a a logical value against each gene to indicate if it has been ranked within the top ngenes_keep. It also generates an additional column dispersion_norm in rowData to store the dispersion metric against each gene.

```{r}
# outlier_dispersion <- isOutlier(dispersion_norm, log=TRUE, type="lower")
# attr(outlier_dispersion, "thresholds")
# threshold <- 0.9390087

# plot density
plot(density(dispersion_norm), main = 'Density - Fano Factor')
# abline(v = threshold)
```

## Structure Preserving Sampling

Performs sampling from the primary clusters in an inverse exponential order of cluster size.

```{r}
set.seed(123)
sce_sampling<-Sampling(sce_HVG, nsamples = 500, method = "sps",
  optm_parameters = FALSE, pinit = 0.195, pfin = 0.9, K = 500)
keep_sampling<-colData(sce_sampling)[,"Sampling"]
sce_sampling<-sce_sampling[,keep_sampling]
```

Sampling in inverse proportion of cluster size following a exponential decay equation. To ensure selection of sufficient representative transcriptomes from small clusters, an exponential decay function is used to determine the proportion of transcriptomes to be sampled from each cluster. For $i^th$ cluster, the proportion of expression profiles $p_i$ was obtained as follows.
pi = pl - e-(Si)/(K) where S_i is the size of cluster i, K is a scaling factor, p_i is the proportion of cells to be sampled from the $i^th$ Louvain cluster. $p_l$ and $p_u$ are lower and upper bounds of the proportion value respectively.

A SingleCellExperiment object with an additional column named Sampling in colData column. The column stores a a logical value against each cell to indicate if it has been sampled.

## Gene selection based on PCA

Performs gene selection on sampled cells based on PCA loadings

```{r}
sce_PCA2<-runPCA(sce_sampling,ntop=1225)
str(reducedDim(sce_PCA2, "PCA"))
dim(reducedDim(sce_PCA2, "PCA"))
percent.var <- attr(reducedDim(sce_PCA2), "percentVar")
plot(percent.var, xlab="PC", ylab="Variance explained (%)")
```

```{r}
# Percentage of variance explained is tucked away in the attributes.
chosen.elbow <- PCAtools::findElbowPoint(percent.var)
chosen.elbow
plot(percent.var, xlab="PC", ylab="Variance explained (%)")
abline(v=chosen.elbow, col="red")
```

```{r}
# Creating a new entry with only the first 20 PCs, 
# useful if we still need the full set of PCs later. 
reducedDim(sce_PCA2, "PCA.elbow") <- reducedDim(sce_PCA2)[,1:chosen.elbow]
reducedDimNames(sce_PCA2)
```

```{r}
loading_top_7 <- attr(reducedDim(sce_PCA2), "rotation")[,1:chosen.elbow]
average_loading <- rowMeans(loading_top_7)
average_loading <- abs(average_loading)
PCA_outlier <- isOutlier(average_loading, type="both")
plot(density(average_loading), main = 'First 7 Components')
abline(v=attr(PCA_outlier, "thresholds"), col="red")
```


```{r}
keep_prin <- (average_loading > 0.003)
table(!keep_prin)

rowData(sce_PCA2)[,"PCAGenes"] <- !keep_prin
```

```{r}
keep_PCA<-rowData(sce_PCA2)[,"PCAGenes"]
sce_PCA<-sce_PCA2[keep_PCA, ]
```

```{r}
dim(sce_PCA)
```


## Clustering

```{r}
set.seed(123)
sce_cluster<-Cluster(sce_PCA, use.subsamples = TRUE, method = "louvian", k_nn = 10, conf = 0.75)
```

Clustering is carried out in two alternate approaches on the sampled cells. For the default setting or quick identification of the existing broad clusters, a Louvain based partition is employed. Otherwise for fine-tuned clustering with outliers, hierarchical clustering is used with cutreeDynamic for dendrogram cut. Also, Assigns cluster membership to unsampled cells by using cluster membership information of the nearest neighbours. An approximate nearest neighbour graph is constructed out of the samples population using the find_ann() module. Some cells are left un-assigned when its neighbour's cluster membership doesn't form a majority as specified by the conf parameter. Unassigned cells (NA) are excluded in the plot or further downstream analysis.

## Visualizing clusters

```{r}
set.seed(123)
sce_plot<-PlotEmbedding(sce_cluster, embedding = "umap", spread = 10, min_dist = 0.1)
plot_data = data.frame("Y1" = reducedDim(sce_plot,"umap")[,1], Y2 = reducedDim(sce_plot, "umap")[,2], color = sce_plot$ClusterIDs)
```

```{r}
ScatterPlot(sce_plot,title = "Clusters")
```


## Metric

```{r}
clusterIDs <- colData(sce_cluster)[,"Sample_ClusterIDs"]
data <- matrix(counts(sce_cluster),nrow = length(clusterIDs))
Dist <- dist(data,method="euclidean")
x <- dunn(clusters=as.vector(clusterIDs),distance = Dist)
print(x)
```

Description -> Calculates the Dunn Index for a given clustering partition.

*dunn(distance = NULL, clusters, Data = NULL, method = "euclidean")*

distance -> The distance matrix (as a matrix object) of the clustered observations. Required if Data is NULL.

clusters -> An integer vector indicating the cluster partitioning

Data -> The data matrix of the clustered observations. Required if distance is NULL.

method-> The metric used to determine the distance matrix. Not used if distance is provided.

*The Dunn Index is the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance. The Dunn Index has a value between zero and infinity, and should be maximized. For details see the package vignette.*